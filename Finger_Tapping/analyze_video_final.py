import os, time, argparse, datetime as dt
import numpy as np
import pandas as pd
import cv2 as cv

# ---------------- feature_extraction ê°€ì ¸ì˜¤ê¸° + ì „ì—­ ì‹¬ë³¼ í•«í”½ìŠ¤ ----------------
from feature_extraction import get_final_features, distance
import feature_extraction as fe  # ëª¨ë“ˆ ê°ì²´

# iqr ë¶€ì¬ í•«í”½ìŠ¤ (NaN-safe)
if not hasattr(fe, "iqr"):
    def iqr(a):
        import numpy as _np
        a = _np.asarray(a, dtype=float)
        if a.size == 0:
            return float("nan")
        q75 = _np.nanpercentile(a, 75)
        q25 = _np.nanpercentile(a, 25)
        return float(q75 - q25)
    fe.iqr = iqr

# scikit-learn ì‹¬ë³¼ ì£¼ì… (LinearRegression, r2_score, PolynomialFeatures)
try:
    from sklearn.linear_model import LinearRegression as _LR
    from sklearn.metrics import r2_score as _r2
    try:
        from sklearn.preprocessing import PolynomialFeatures as _PF
    except Exception:
        _PF = None
except Exception:
    _LR = _r2 = _PF = None

if not hasattr(fe, "LinearRegression"):
    if _LR is None:
        raise SystemExit(
            "scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤. í™œì„± venvì—ì„œ\n"
            "  pip install scikit-learn\n"
            "ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”."
        )
    fe.LinearRegression = _LR
if not hasattr(fe, "r2_score") and _r2 is not None:
    fe.r2_score = _r2
if _PF is not None and not hasattr(fe, "PolynomialFeatures"):
    fe.PolynomialFeatures = _PF
# ---------------------------------------------------------------------------

# MediaPipe
import mediapipe as mp
Hands = mp.solutions.hands.Hands
HAND_CONNECTIONS = mp.solutions.hands.HAND_CONNECTIONS
draw = mp.solutions.drawing_utils
from mediapipe.framework.formats import landmark_pb2

# Pillow(í•œê¸€)
try:
    from PIL import ImageFont, ImageDraw, Image
except Exception:
    ImageFont = ImageDraw = Image = None

# ----------------- ìœ í‹¸ -----------------
def ensure_dir(p): os.makedirs(p, exist_ok=True); return p
def now_tag(): return dt.datetime.now().strftime("%Y%m%d_%H%M%S")

def noisyor(probs: np.ndarray) -> float:
    """Noisy-OR ê²°í•© í™•ë¥  ê³„ì‚°"""
    p = np.clip(np.asarray(probs, dtype=float), 0.0, 1.0)
    return float(1.0 - np.prod(1.0 - p))

def angle_deg(ax, ay, bx, by):
    dot = ax*bx + ay*by
    na = np.hypot(ax, ay); nb = np.hypot(bx, by)
    if na == 0 or nb == 0: return -1.0
    c = np.clip(dot/(na*nb), -1.0, 1.0)
    return float(np.degrees(np.arccos(c)))

def write_landmarks_csv(outdir, tag, hand_label, frames):
    rows = []
    for i, lm in enumerate(frames):
        row = {"frame": i, "hand": hand_label.upper()}
        if lm is None:
            for j in range(21):
                row[f"x_{j}"]=np.nan; row[f"y_{j}"]=np.nan; row[f"z_{j}"]=np.nan
        else:
            for j, l in enumerate(lm):
                row[f"x_{j}"]=l.x; row[f"y_{j}"]=l.y; row[f"z_{j}"]=l.z
        rows.append(row)
    pd.DataFrame(rows).to_csv(os.path.join(outdir, f"landmarks_{tag}_{hand_label.upper()}.csv"), index=False)

def collect_timeseries(session_landmarks, frame_size, duration_s):
    w, h = frame_size
    per_hand = {}

    for hand, frames in session_landmarks.items():
        D_raw, W_raw = [], []

        for lm in frames:
            if lm is None:
                D_raw.append(np.nan)
                W_raw.append((np.nan, np.nan))
                continue

            w_lm, cmc, th, idx = lm[0], lm[1], lm[4], lm[8]
            wx, wy = int(w_lm.x * w), int(w_lm.y * h)
            tx, ty = int(th.x * w), int(th.y * h)
            ix, iy = int(idx.x * w), int(idx.y * h)

            ang = angle_deg(tx - wx, ty - wy, ix - wx, iy - wy)
            D_raw.append(ang)

            cmx, cmy = int(cmc.x * w), int(cmc.y * h)
            wnorm = distance(wx, wy, cmx, cmy) or np.nan
            W_raw.append((wx / wnorm, wy / wnorm) if not np.isnan(wnorm) else (np.nan, np.nan))

        df_tmp = pd.DataFrame({
            "D_raw": D_raw,
            "W_raw_x": [w[0] for w in W_raw],
            "W_raw_y": [w[1] for w in W_raw]
        })
        df_tmp = df_tmp.interpolate(limit_direction="both")

        D_raw = df_tmp["D_raw"].to_numpy()
        W_raw = list(zip(df_tmp["W_raw_x"], df_tmp["W_raw_y"]))

        per_hand[hand] = {
            "D_raw": D_raw,
            "W_raw": W_raw,
            "num_frames": len(frames),
            "duration": float(duration_s)
        }

    return per_hand

# -------------- í•œê¸€ í…ìŠ¤íŠ¸ --------------
def _auto_korean_font():
    cands = [
        r"C:\Windows\Fonts\malgun.ttf", r"C:\Windows\Fonts\malgunbd.ttf",
        r"C:\Windows\Fonts\NanumGothic.ttf", r"C:\Windows\Fonts\gulim.ttc",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
    ]
    for p in cands:
        if os.path.exists(p): return p
    return None

def put_kr(img_bgr, text, org, font_path=None, font_size=26,
           color=(255,255,255), stroke_color=(0,0,0), stroke_width=2):
    if ImageFont is None:
        cv.putText(img_bgr, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, cv.LINE_AA)
        return img_bgr
    if not font_path or not os.path.exists(font_path):
        font_path = _auto_korean_font()
    try:
        font = ImageFont.truetype(font_path, font_size) if font_path else None
    except Exception:
        font = None
    if font is None:
        cv.putText(img_bgr, text, org, cv.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, cv.LINE_AA)
        return img_bgr

    def bgr2rgb(c): return (int(c[2]), int(c[1]), int(c[0]))
    img_rgb = cv.cvtColor(img_bgr, cv.COLOR_BGR2RGB)
    pil_img = Image.fromarray(img_rgb); draw_pil = ImageDraw.Draw(pil_img)
    if stroke_width>0:
        draw_pil.text(org, text, font=font, fill=bgr2rgb(stroke_color),
                      stroke_width=stroke_width, stroke_fill=bgr2rgb(stroke_color))
    draw_pil.text(org, text, font=font, fill=bgr2rgb(color),
                  stroke_width=stroke_width, stroke_fill=bgr2rgb(stroke_color))
    return cv.cvtColor(np.array(pil_img), cv.COLOR_RGB2BGR)

def draw_panel(img, x, y, w, h, alpha=0.35, color=(0,0,0)):
    overlay = img.copy()
    cv.rectangle(overlay, (x, y), (x+w, y+h), color, thickness=-1)
    return cv.addWeighted(overlay, alpha, img, 1-alpha, 0)

# -------------- ë¯¸ëŸ¬ í‘œì‹œìš© --------------
def draw_landmarks_mirrored(frame_disp, lm_list, connections):
    mirrored = landmark_pb2.NormalizedLandmarkList(
        landmark=[landmark_pb2.NormalizedLandmark(x=1.0 - l.x, y=l.y, z=l.z) for l in lm_list]
    )
    draw.draw_landmarks(frame_disp, mirrored, connections)

# ----------------- ë©”ì¸ -----------------
def main():
    ap = argparse.ArgumentParser(description="(Lite) Webcam â†’ Record â†’ Analyze Video â†’ Final features CSV (Mirror+KR)")

    # ==== ì¸ì ì •ì˜ ====
    ap.add_argument("--artifact_path", type=str,
                    default="./pd_hc_binary/best_pipeline_auc_AdaBoost.joblib",
                    help="ì €ì¥ëœ íŒŒì´í”„ë¼ì¸(.joblib) ë˜ëŠ” ë²ˆë“¤(dict) ê²½ë¡œ")
    ap.add_argument("--camera", type=int, default=0)
    ap.add_argument("--outdir", type=str, default="./outputs")
    ap.add_argument("--schema_csv", type=str, default="./severity_dataset_dropped_correlated_columns_modf.csv")
    ap.add_argument("--prep_seconds", type=int, default=5)
    ap.add_argument("--taps", type=int, default=10)
    ap.add_argument("--max_seconds", type=int, default=180)
    ap.add_argument("--tap_on_ratio", type=float, default=0.035)
    ap.add_argument("--tap_off_ratio", type=float, default=0.06)
    ap.add_argument("--font", type=str, default="")
    ap.add_argument("--swap_labels_for_display", action="store_true", default=True)
    ap.add_argument("--thresh", type=float, default=0.50, help="ê¸°ë³¸ PD íŒì • ì„ê³„ê°’")
    ap.add_argument("--delta", type=float, default=0.05, help="ì˜ë¡œì¡´ í­")
    ap.add_argument("--video_path", type=str, default="", help="ë¶„ì„í•  ì˜ìƒ íŒŒì¼ ê²½ë¡œ")
    ap.add_argument("--strategy", type=str, choices=["recall", "youden", "accuracy", "default"],
                    default="default", help="threshold ê¸°ì¤€ ì„ íƒ")
    args = ap.parse_args()

    # ==== ì¶œë ¥ í´ë” ë° íŒŒì¼ëª… ====
    outdir = ensure_dir(args.outdir)
    schema_cols = list(pd.read_csv(args.schema_csv, nrows=1).columns)
    tag = now_tag()
    features_fname = f"features_{tag}.csv"
    font_path = args.font if args.font else _auto_korean_font()

    # ==== 1. ë…¹í™” or ê¸°ì¡´ ë¹„ë””ì˜¤ ì‚¬ìš© ====
    recorded_video_path = args.video_path
    if not recorded_video_path:
        recorded_video_path = os.path.join(outdir, f"recording_{tag}.mp4")

        cap = cv.VideoCapture(args.camera)
        if not cap.isOpened():
            raise RuntimeError(f"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --camera {args.camera}")

        w = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))
        h = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv.CAP_PROP_FPS)) or 20
        fourcc = cv.VideoWriter_fourcc(*'mp4v')
        video_writer = cv.VideoWriter(recorded_video_path, fourcc, fps, (w, h))
        print(f"[INFO] ë…¹í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {recorded_video_path}")

        hands_for_preview = Hands(static_image_mode=False, max_num_hands=2,
                                  model_complexity=1, min_detection_confidence=0.6,
                                  min_tracking_confidence=0.8)

        prep_start = time.time()
        while True:
            ok, frame = cap.read()
            if not ok:
                raise RuntimeError("ì¹´ë©”ë¼ í”„ë ˆì„ ìˆ˜ì‹  ì‹¤íŒ¨")
            video_writer.write(frame)
            remain = args.prep_seconds - (time.time() - prep_start)
            if remain < 0:
                break
            frame_disp = cv.flip(frame.copy(), 1)
            frame_disp = put_kr(frame_disp, "ì¤€ë¹„ ì¤‘ (ê±°ìš¸ í™”ë©´)", (10, 40), font_path, 28, (0,255,255))
            frame_disp = put_kr(frame_disp, f"ì‹œì‘ê¹Œì§€: {int(np.ceil(remain))}ì´ˆ", (10, 80), font_path, 26, (0,255,255))
            cv.imshow("Hand Capture (Mirror) - Recording...", frame_disp)
            if cv.waitKey(1) & 0xFF == ord('q'):
                cap.release(); video_writer.release(); cv.destroyAllWindows(); return

        last_release, count = {"Left": True, "Right": True}, {"Left": 0, "Right": 0}
        min_side, meas_start = None, time.time()

        # [FIX 1] ì¸¡ì • ë£¨í”„ì— í™”ë©´ í‘œì‹œ(imshow) ë° ì‹œê°í™” ë¡œì§ ì¶”ê°€
        while True:
            ok, frame = cap.read()
            if not ok: break
            video_writer.write(frame)
            h, w = frame.shape[:2]
            if min_side is None: min_side = min(w, h)
            on_th, off_th = min_side*args.tap_on_ratio, min_side*args.tap_off_ratio

            frame_disp = cv.flip(frame.copy(), 1)
            rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
            res = hands_for_preview.process(rgb)

            if res.multi_hand_landmarks and res.multi_handedness:
                for i, hd in enumerate(res.multi_handedness):
                    label = hd.classification[0].label
                    score = hd.classification[0].score
                    if score < 0.5: continue
                    lm = res.multi_hand_landmarks[i].landmark
                    
                    draw_landmarks_mirrored(frame_disp, lm, HAND_CONNECTIONS)

                    tx, ty = int(lm[4].x*w), int(lm[4].y*h)
                    ix, iy = int(lm[8].x*w), int(lm[8].y*h)
                    dpx = float(np.hypot(tx-ix, ty-iy))
                    touching = dpx < on_th
                    if last_release[label] and touching:
                        count[label] += 1
                        last_release[label] = False
                    if dpx > off_th:
                        last_release[label] = True

            panel_h = 78
            frame_disp = draw_panel(frame_disp, 6, h - panel_h - 6, w - 12, panel_h, alpha=0.30, color=(0,0,0))
            elapsed = time.time() - meas_start
            lc_true = min(count["Left"],  args.taps)
            rc_true = min(count["Right"], args.taps)
            
            if args.swap_labels_for_display: lc_disp, rc_disp = rc_true, lc_true
            else: lc_disp, rc_disp = lc_true, rc_true

            frame_disp = put_kr(frame_disp, f"ì‹œê°„: {elapsed:4.1f}ì´ˆ", (12, h - panel_h - 6 + 26), font_path, 26, (255,255,255))
            frame_disp = put_kr(frame_disp, f"ì™¼ì† {lc_disp}/{args.taps}  |  ì˜¤ë¥¸ì† {rc_disp}/{args.taps}", (12, h - panel_h - 6 + 56), font_path, 26, (255,255,255))
            
            cv.imshow("Hand Capture (Mirror) - Recording...", frame_disp)

            if (count["Left"] >= args.taps) and (count["Right"] >= args.taps): break
            if elapsed >= args.max_seconds: break
            if cv.waitKey(1) & 0xFF == ord('q'): break

        cap.release(); video_writer.release(); cv.destroyAllWindows()
        hands_for_preview.close()
        print("[INFO] ë…¹í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        total_duration = time.time() - meas_start
    else:
        cap_tmp = cv.VideoCapture(recorded_video_path)
        fps = cap_tmp.get(cv.CAP_PROP_FPS)
        num_frames = cap_tmp.get(cv.CAP_PROP_FRAME_COUNT)
        total_duration = num_frames / fps if fps > 0 else 0
        cap_tmp.release()

    # [FIX 2] ëœë“œë§ˆí¬ ì¶”ì¶œ ë° íŠ¹ì§• ê³„ì‚° ë¡œì§ ë³µì›
    print(f"[INFO] ë…¹í™”ëœ ì˜ìƒ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤: {recorded_video_path}")
    cap_analysis = cv.VideoCapture(recorded_video_path)
    if not cap_analysis.isOpened():
        raise RuntimeError(f"ë…¹í™”ëœ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {recorded_video_path}")

    analysis_w = int(cap_analysis.get(cv.CAP_PROP_FRAME_WIDTH))
    analysis_h = int(cap_analysis.get(cv.CAP_PROP_FRAME_HEIGHT))
    num_video_frames = int(cap_analysis.get(cv.CAP_PROP_FRAME_COUNT))
    frame_size = (analysis_w, analysis_h)
    
    hands_for_analysis = Hands(static_image_mode=True, max_num_hands=2,
                               model_complexity=1, min_detection_confidence=0.5)

    session_landmarks = {"Left": [], "Right": []}
    frame_count = 0
    while True:
        ok, frame = cap_analysis.read()
        if not ok: break
        
        frame_count += 1
        print(f"\r[INFO] ì˜ìƒ ë¶„ì„ ì¤‘... {frame_count}/{num_video_frames}", end="")

        rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
        res = hands_for_analysis.process(rgb)
        
        current = {"Left": None, "Right": None}
        if res.multi_hand_landmarks and res.multi_handedness:
            for i, hd in enumerate(res.multi_handedness):
                label = hd.classification[0].label
                current[label] = res.multi_hand_landmarks[i].landmark
        
        session_landmarks["Left"].append(current["Left"])
        session_landmarks["Right"].append(current["Right"])
        
    cap_analysis.release()
    hands_for_analysis.close()
    print("\n[INFO] ì˜ìƒ ë¶„ì„ ì™„ë£Œ.")

    data_by_hand = collect_timeseries(session_landmarks, frame_size, total_duration)
    rows = []
    for label in ["Left", "Right"]:
        try:
            feats = get_final_features(data_by_hand[label])
            feats["hand"] = label.lower()
        except Exception as e:
            print(f"[WARN] {label} í”¼ì²˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            feats = {"hand": label.lower()}
        rows.append(feats)

    df_raw_all = pd.DataFrame(rows)
    df_raw_all["filename"] = features_fname
    for c in schema_cols:
        if c not in df_raw_all.columns:
            df_raw_all[c] = np.nan
    df_infer = df_raw_all[schema_cols].copy()
    
    out_path = os.path.join(outdir, features_fname)
    df_infer.to_csv(out_path, index=False)
    print(f"[SAVED] {out_path}")


    # ==== 3. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì˜ˆì¸¡ ====
    from joblib import load
    from sklearn.pipeline import Pipeline
    
    bundle = load(args.artifact_path)
    
    pipeline = None
    if isinstance(bundle, Pipeline):
        pipeline = bundle
    elif isinstance(bundle, dict) and "pipeline" in bundle:
        pipeline = bundle["pipeline"]
    else:
        raise RuntimeError("ì•„í‹°íŒ©íŠ¸ì—ì„œ íŒŒì´í”„ë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ==== 4. threshold ì „ëµ ì„ íƒ ====
    person_thresh = None
    strategy_used = "manual"

    if isinstance(bundle, dict):
        default_strategy = bundle.get("threshold_strategy", "default")
        default_thresh = bundle.get("person_threshold", 0.5)

        if args.strategy == "default":
            person_thresh = default_thresh
            strategy_used = default_strategy
        else:
            metrics_all = bundle.get("metrics_all", {})
            if args.strategy in metrics_all:
                person_thresh = float(metrics_all[args.strategy].get("threshold", default_thresh))
                strategy_used = args.strategy
            else:
                person_thresh = default_thresh
                strategy_used = default_strategy
    else:
        person_thresh = 0.5

# ==== ì‚¬ìš©ì ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ì¡°ì •ê°’ ì…ë ¥ ====
    THRESH_OFFSET = 0.12   # ğŸ‘‰ ì—¬ê¸°ì„œ ë³´ì •ê°’ì„ ì§ì ‘ ì…ë ¥ (+0.05ë©´ threshold 0.05 ì˜¬ë¦¼, -0.02ë©´ 0.02 ë‚´ë¦¼)
    person_thresh = person_thresh + THRESH_OFFSET

    print(f"[INFO] threshold_strategy='{strategy_used}', "
          f"artifact_base={default_thresh:.4f}, offset={THRESH_OFFSET:+.4f}, "
          f"final_person_threshold={person_thresh:.4f}")


    # ==== pipeline ì˜ˆì¸¡ ====
    y_prob = pipeline.predict_proba(df_infer)[:, 1]
    df_out = df_raw_all.copy()
    df_out["probability"] = y_prob
    # ì† ë‹¨ìœ„ ì˜ˆì¸¡ì€ ìµœì¢… ì„ê³„ê°’ì„ ë”°ë¦„
    df_out["prediction"] = (df_out["probability"] >= person_thresh).astype(int)

    # ==== ì† ë‹¨ìœ„ ê²°ê³¼ (Zone ê¸°ë°˜) ====
    zones = {}
    for _, row in df_out.iterrows():
        prob = row['probability']
        hand = row['hand']

        if prob < (person_thresh - args.delta):
            zone = "green"
            label = "ì •ìƒ(HC)"
        elif prob < (person_thresh + args.delta):
            zone = "yellow"
            label = "ì£¼ì˜/ì¬ê²€ ê¶Œìœ "
        else:
            zone = "red"
            label = "í™˜ì(PD)"

        zones[hand] = zone
        print(f"[RESULT] {hand}: {label} (ìœ„í—˜ë„={prob:.2f}, TH={person_thresh:.2f}, Zone={zone.upper()})")

    pred_path = out_path.replace("features_", "predictions_")
    df_out.to_csv(pred_path, index=False)
    print(f"[INFO] Saved predictions to {pred_path}")

    # ==== ì‚¬ëŒ ë‹¨ìœ„ ìµœì¢… íŒì • (ì† ë‹¨ìœ„ Zone ì¢…í•©) ====
    if all(z == "green" for z in zones.values()):
        final_pred = 0
        zone = "green"
        diagnosis = "ì •ìƒ(HC)"
    elif any(z == "red" for z in zones.values()):
        final_pred = 1
        zone = "red"
        diagnosis = "í™˜ì(PD)"
    else:
        final_pred = -1
        zone = "yellow"
        diagnosis = "ì£¼ì˜/ì¬ê²€ ê¶Œìœ "

    print("-" * 50)
    print(f"[PERSON-LEVEL PREDICTION]")
    print(f"  - ìµœì¢… ì§„ë‹¨: {diagnosis} (Zone: {zone.upper()})")
    print(f"  - ì™¼ì† Zone: {zones.get('left')}, ì˜¤ë¥¸ì† Zone: {zones.get('right')}")
    print(f"  - ì ìš© ì „ëµ: '{strategy_used}' (ì„ê³„ê°’: {person_thresh:.4f}, Î”={args.delta})")
    print("-" * 50)

    per_person = pd.DataFrame([{
        "filename": features_fname,
        "final_pred": final_pred,   # -1=ì£¼ì˜, 0=ì •ìƒ, 1=PD
        "left_zone": zones.get("left"),
        "right_zone": zones.get("right"),
        "strategy": strategy_used,
        "threshold_used": person_thresh
    }])
    person_path = pred_path.replace("predictions_", "person_level_decision_")
    per_person.to_csv(person_path, index=False)
    print(f"[INFO] Saved person-level decision to {person_path}")


if __name__ == "__main__":
    main()